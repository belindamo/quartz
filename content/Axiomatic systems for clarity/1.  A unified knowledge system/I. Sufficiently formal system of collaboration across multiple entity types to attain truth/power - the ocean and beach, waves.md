*Sometimes I think about how fucked up Silicon Valley is. It's a place of power. It is full of greed - of people trying to prove themselves and ignoring the needs of other people along the way. They look at the future - at AGI, at crypto - without clarity of how it might help humans in the here and now.* 

# the ocean and wave

Some humans deal in different timescales and that is where conflict occurs.

Some Silicon Valley CEO talks about longevity, with rumors that they get blood infusions from young people. Today people think they're a vampire. In 100 years, perhaps it is the norm.

Generally, those who have enough resources today think about longer timescales. Those who are more resource strapped are required to tink in shorter timescales for their own survival. 

Person A is thinking about the ocean but Person B only sees the waves. B is the one who feels the greatest impact of the waves. A does not feel the impact but sees that if a tsunami occurs then the ocean might be improved. It may entirely destroy Person B's zone, but A is saving the ocean for the longterm.

This also reminds me of human sacrifices in old civilizations. Humans will sacrifice loved ones to appease deities. It was "for the greater good". Yet, we learn a few civilizations later that there is no measurable correation between natural disasters and human sacrifices. Person A was wrong and B suffered the consequences. 

Worse yet is if B is wrong *and* knows it. Due to self-interest, they go ahead with the human sacrifice anyways. Who checks B when this happens? How are they punished for breaking their social contract with A?

A is naturally held to higher standards because A:B is a one to many relationship. The greater the difference in scope, the more Bs there are to an A. 

Dfference in scope occurs both in time and amount of environment and people impacted.

---
How might we apply math and science to how we live our lives? 

Engineers and scientists do this somewhat organically. given their training.

** missing a page **

---

someone else's life. A, thinking of the oceam, must make tradeoffs given resource constraints. Some tradeoffs are conscious. In other instances, the error rate of a decision leads to unexpected errors that are statistically likely to happen because airtight "if x, then y" logic is almost never 100% true in the real world. 

Conflict occurs between A and B for a few reasons. 

1. Either A or B doesn't understand that resource constraints, tradeoffs, and statistical errors will inevitably occur. This may be due to lack of prior education in scientific fields. The significantly worse reason is that the part understands these principles and un/consciously rejects them in favor of unreproducible philosophy, i.e. religion & spiritual realm beliefs. This second reason isa tough one to handle, and I think the buck stops here if either A or B refuses to acknowledge logic and engineering principles when working together. [1]


[1] That being said, spiritual and religious beliefs are not mutually exclusive to scientific thought. But for human-influenced decision making we should assume decisions based on what is observable and reproducible. There may be some cases to take belief into account as a variable, in which case we are still making logical decisions taking into account [belief, emotion, unobservable phenomena] as variables
^How to take these into account as variables? 


Assuming then that A and B understand that resource constraints / tradeoffs / statistical error exists, then there may be other reasons they may not reach consensus of be annoyed at each other.


1.5. A or B are not benevolent agents. They are breaking social contract in some way, typically out of self interest. (conscious or unconscious) The other part likes to default to thinking 1.5 is true, or that they'll use 1.5 as an argument. I think it's less true that people assume generally. 

2. Different weighting. B will almost always weight the waves more highly than A. This is recency bias, which might be conscious but is more likely than not unconscious. B knows more details of the waves and their entire context window is waves. For A, their entire context is ocean, and waves are only one component. Scope difference naturally leads to A weighting B's needs less than B does. Even given A and B are benevolent agents doing their best and that understand tradeoffs, this tension will naturally exist betwen A and B.

3. Information loss. B lacks A's broader context and A lacks B's level of detail. Differently scoped context windows leads to lost information. A and B are making decisions with incomplete information, relative to the other party. 

By system necessity, A may have to withhold some info. A and B may both not feel comfortable sharing all information. They may also not even know that the other party isn't aware of that info. The info may not easily be imparted through words.

Even if A and B are in the same room, fully transparent, talking together, [words are compression algorithms](https://alexw.substack.com/p/information-compression). Words carry different meanings to different groups of people, depending on context. 


3.1 compression loss
3.2 knowledge withheld by necessity


These are reasons why there may be conflict.

**If we take each reason as a variable and are able to talk through each variable, then driving consensus is simply a decision tree / series of logic gates.**

Decision-specific variables may include: emotion, intuition, happiness, health, etc. Even "fuzzy" topics can be variables. An important consideration here though is that a fuzzier variable takes longer to determine the formula and metric for, and the fuzzier variable will inevitably be less accurate. It may have multiple layers unto itself. The formula / metric and variable weight both need to be discussed to gain consensus. The fuzzier the variable, the longer is appropriate and needed to discuss.

All in all, this can be applied to action (decision) and thought (opinion).

We can disagree about action and thought and still be friends!

Ideally we move forward with consensus [2]. Even if there is disagreement, at least we can pinpoint the reason (1-3) and/or variable that caused it.

[2] how to do consensus in fuzziness?

Preliminary thought: this is not possible withou a baseline level of trust (1.5, 3.2) If we assumed everyone in the system is benevolent, then we can have clear consensus or reason for disagreement.

---
What do we expect of people over time? People are remarkably consistent.

If we are to write more about human nature, like Nietzche, Aristotle, Locke, then we must take into account what we know more of than in their time periods. We take what is still true from their periods, i.e. logic and rhetoric. They also took in human observation

Yet, what we are also exploring on top of that is statisticaly methods, computational logic, engineering system principles, neuroscience, AI principles. In order to have updated, fresh view on the world and people, we must take into account updated bodies of knowledge. This is as accurate as we can get given what we know now.

How might we get to truths that are bullet proof? How might we apply the art of mathematical proofs (i.e. logic) to human systems? 

There is a range of certainty levels for various tools in your mental toolkit:

From greatest to least certainty:
- Math proof / logic gates
	- Conditioned on every variable being true or false.
- Probabiltiy / staistical methods (I think AI models fall here. Maybe uncorrelative?)
- Correlative methods (statistical significance)
- 1st person qualitative accounts (small n, statistical insignificance)

I would call these moreso high-levle processes. In reality, when considering each tool, variables plugged into each tool will vary widely in fuzziness. 

I think it'd be helpful to show each of these tools used and applied to real life thoughts and actions. For example, how might we decide to choose a job, stay with an SO, or move to another city? 

Generally, the fuzzier the variable, such as emotional state, the harder it is to measure. 

Variables can be binary, scale, [0,1], single vectors, or multidimensional vectors.

For human understandability, it's usually better to consider variables as binary or scale. Vectors are much harder to garner insights / meaning from within a human mind, although they're useful for computer models. Vectors as variables also require gathering quite a lot of data which we don't always have on hand.

Usually, issues come about when people mistake 1 method for another, and expect the outcome of that tool to be that of another tool. For instance, psychology is largely founded on correlative studies so it must be cited with caution. They might imply misleading certainty levels and in some cases [[Personality tests|not be accurate at all]]. 

Another risk using these tools is of how we might measure each variable. When we say the devil's in the details, this is a common pitfall of applied math/ML. (ex: BLEU)
In fact, this risk may be the largest one. [3] The way we measure variables over time, especially fuzzier ones. This is "direction" - which way are we optimizing to reach for?

Also the ways we gain consensus on the process and variables. [4] How might we? 



[3] Examples:
- Athena, 4 pgs of output vs 2
- burger, eat vs not impulse control
- job offer (salary, prestige, what if there's something better?) (salary -> is there a set for think vs feel -> weight each -> lack of alignment comes from incorrect weighting from what body wants)


[4] sets of thinking vs feeling? 

---

https://docs.google.com/document/d/1TxttChfVL8JnljIKEaC28LdeDqHEUnP3gSyrbYtcA2c/edit


- can we get UN to agree on certain logical principles? 

- context windows....

related: [pace layering](https://jods.mitpress.mit.edu/pub/issue3-brand/release/2)


ACTION ITEMS
flesh out further...
- what this toolkit looks like
- how consensus might be reached on how to measure variables, weight them, and sustain context over different window sizes/locales



Proof by contradiction, direct, etc. 



Discernment <-> execution ability. what humans are still more useful for...
	GPT-3, LLM for idea genreation.
	Humans for discernment?


justifying scope difference is a slippery slope to evil
same w colllective v indivdual good


![[image 2 1.jpg]]