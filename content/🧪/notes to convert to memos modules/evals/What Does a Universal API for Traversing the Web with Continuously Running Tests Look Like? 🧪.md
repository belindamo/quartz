---
tags:
  - building-block
  - joy
  - external-experiment
type:
  - code
  - human-ai-comparison
claim type:
  - x>y
theme:
  - agents
  - duplicate-self
est. time: Two weeks
actual time: 
problem: Ensuring the reliability of AI-powered browser automation responses, irrespective of the method used.
bit: Current reliance on human monitoring for successful task completion.
flip: Shift responsibility for task verification to continuously running and updating tests.
instantiate solution: AI-driven automation for web traversal, adaptable for various applications, with continuous testing to ensure functionality.
method: 1. Use a large language model for AI-generated automation in website traversal, suitable for cloud or local deployment. 2. Continuous testing (e.g., GitHub Actions) for task consistency and functionality. 3. Crowdsourced task generation and submission. 4. Automatic AI regeneration of browser automation upon test failures.
evaluation:
  - Error consistency in continuous testing.
  - Success rate of AI script regeneration (target â‰¥ 90%).
  - Effectiveness on sites with sensitive information or bot detection.
implications: Scalability from one to thousands of tasks, potential for crowdsourcing and community funding.
related works: "  - Multi-on: An assistant platform.  - Selenium and Puppeteer: Browser automation tools."
eval plan: "- IV: AI in browser automation creation and continuous testing setup.  - DV: Task effectiveness without user intervention.  - Task: As outlined in method.  - Threats: Inability to capture all errors in tests and AI-generated tree."
findings: 
pass/fail?: 
what did I learn?:
---




- re-order contacts
- schedule a medical appointment
- automatically write drafts of my emails
