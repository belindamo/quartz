---
date: 2024-01-23
title: RAG for an orthopedic surgeon
---


![[Pasted image 20240205182916.png]]
## **Experiment Length:**

**Estimated Time:**Â 1 day  
**Actual Time:**Â 1 day

## **Summary**

The crux of this experiment was to address the challenge that my brother, an orthopedic surgeon, faces: spending 10+ hours weekly on manual research literature searches for patient cases.

Traditional AI tools like ChatGPT andÂ [Semantic Scholar](https://www.semanticscholar.org/)Â weren't cutting it due to broad responses, outdated or irrelevant articles. The bit here is that existing research tools cater to consumers or researchers, not practitioners like surgeons. The flip involved using RAG (Retrieval-Augmented Generation) on tailored data sources from the orthopedic community, combined with GPT-4, to enhance response quality. The solution was instantiated using LlamaIndex for RAG with embeddings from selected reputable sources. We aimed to see if this tailored approach was more effective for a field-specific practitioner.

## **Plan**

The method was straightforward:

1. A medical expert (my brother) selected relevant medical data sources, exported as PDFs.
2. These PDFs were converted into an embeddings file using LlamaIndex, stored locally.
3. Use LlamaIndex's engine to perform RAG on queries, with an emphasis on citation references.
4. The quality of answers was tested by my brother.
5. The project was hosted on OnRender, built with Next.js/Tailwind, guided by LlamaIndex's blog.

The evaluation plan centered around the qualitative analysis of answers to typical patient case questions. For example, "What are the demographics of people who get proximal humerus fractures?" Implications are broad, potentially benefiting any medical field with tailored resource curation.

## **Result: ðŸŸ¢ Pass**

### Example 1 (easy mode)

**RAG + GPT-4:**

![[qWvtA4F_ornc2_Royq2av.png]]

**GPT-4 only:**

![[7V9v_sly-6I8YSRAamGef.png]]

### Example 2 (medium-hard mode)

**RAG + GPT-4:**

![[wqpm8wSfe9OMO8zt4I1g2.png]]

**GPT-4 only:**

![[fT8jErVxDsb1KMmTtndcD.png]]

Wow, we were really blown away by the quality of the answers from LlamaIndex! The answers from RAG + GPT-4 were very straight to the point and gave specific details that a surgeon would look out for. GPT-4 also gave a much "fluffier" and long-winded response, that was sometimes completely wrong. In Example 2, GPT-4 gave the unhelpful response that the two demographics of "Older Adults" and "Young Adults" were the most prone to proximal humerus fractures.

That being said, we were looking for differences from a surgeon's expertise, and we definitely saw limits. When we stress tested some questions, especially the ones that would need info from various sources, they didn't all hit the mark.

Another main issue? Speed. It took over 10 seconds to get a response, which feels like forever when you're waiting. This was mainly because we were dealing with about 60MB of dataÂ _stored locally_, which is a lot to process on the fly.

Here's what I'd do differently next time:

- Ditch the local PDF embeddings. They were a big part of why things were so slow. Next time, I'm thinking of using something like Pinecone for storing vector embeddings. I'd also probably markdown or text files rather than pdf.
- Tackle the formatting issues in our PDFs. Badly formatted text led to some less-than-stellar answers. Maybe OCR or starting with cleaner data formats would help.
- Use LlamaIndex's CitationQueryEngine. I only had a brief play with this, but it seemed like a game-changer for getting better references in our answers. Definitely on my list for the next round.

Overall, a thumbs up for the experiment, with some good takeaways for making it even better next time! I'm confident that these three improvements would make a literature review search that would cut a surgeon's literature search time in half.

## **What I Learned**

This experiment was a deep dive into LlamaIndex and its capabilities, and broadly RAG. The potential of tailored AI tools in medical research is immense. The key is proper data curation and choosing the right tools for efficient and accurate retrieval-augmented generation.

---

_This blog post was generated by combining my messy notes with ChatGPT ;) The title image was generated using Midjourney with the blog post title as the prompt._
