#concept-pamphlet 

What is micrograd? Why is it called micrograd?
[[SR/memory/GFesQRCn.md|?]]
A simple repo by Andrej Karpathy to show how neural networks are trained under the hood in a very simple way.
It is a scalar-valued autograd, or automatic gradient/differentiation calculator. It takes out any efficiency complexities like using matrices.


### References
1. https://www.youtube.com/watch?v=VMj-3S1tku0

### Notes

- you can even do backpropagation on tan h only. You can do backpropagation on any composite function

