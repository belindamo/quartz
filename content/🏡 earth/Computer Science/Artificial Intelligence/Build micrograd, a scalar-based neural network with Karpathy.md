#concept-pamphlet 

What is micrograd? Why is it called micrograd?
?
A simple repo by Andrej Karpathy to show how neural networks are trained under the hood in a very simple way.
It is a scalar-valued autograd, or automatic gradient/differentiation calculator. It takes out any efficiency complexities like using matrices.
<!--SR:!2024-11-01,97,290-->

### References
1. https://www.youtube.com/watch?v=VMj-3S1tku0

### Notes

- you can even do backpropagation on tan h only. You can do backpropagation on any composite function

