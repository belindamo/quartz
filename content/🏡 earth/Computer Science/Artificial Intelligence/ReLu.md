#concept

What is **ReLu**?
?
Rectified Linear Unit. It is a common type of activation function.
`f(x)=max(0,x)`
- For xâ‰¥0: The function returns x (i.e., the function behaves linearly).
- For x<0: The function returns 0.
<!--SR:!2024-10-04,12,270-->

### References
1. [[Build micrograd, a scalar-based neural network with Karpathy]]
<!--SR:!2024-11-08,52,190-->

### Notes




