#concept

What is **ReLu**?
?
Rectified Linear Unit. It is a common type of activation function.
`f(x)=max(0,x)`
- For xâ‰¥0: The function returns x (i.e., the function behaves linearly).
- For x<0: The function returns 0.
<!--SR:!2024-08-24,28,190-->
### References
1. [[Build micrograd, a scalar-based neural network with Karpathy]]

### Notes




