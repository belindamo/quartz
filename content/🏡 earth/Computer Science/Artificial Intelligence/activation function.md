#concept 

activation function
?
calculates an output based on input(s)
Used as a last step of neural network to normalize the output 
source: https://en.wikipedia.org/wiki/Activation_function
<!--SR:!2024-09-27,5,230-->

Activation functions include
- 1 fold x from previous layers
	- [[ReLu]]
	- [[tan h]]
- for a node
	- [[softmax]]


