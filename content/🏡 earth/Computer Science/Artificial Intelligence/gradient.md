#concept


What is a **gradient**? What is it in the context of neural nets?
?
A gradient is a vector containing partial derivatives of an output with respect to its inputs
In neural nets, it quantifies how the neurons relate to the final output
<!--SR:!2024-10-21,34,230-->

### References
1. [[Build micrograd, a scalar-based neural network with Karpathy]]

### Notes




