#concept


What is a **gradient**? What is it in the context of neural nets?
?
A gradient is a vector containing partial derivatives of an output with respect to its inputs
In neural nets, it quantifies how the neurons relate to the final output
### References
1. [[Build micrograd, a scalar-based neural network with Karpathy]]
<!--LEARN:Dt7WNTJn-->

### Notes




